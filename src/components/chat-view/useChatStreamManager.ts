import { UseMutationResult, useMutation } from '@tanstack/react-query'
import { Notice } from 'obsidian'
import { useCallback, useMemo, useRef } from 'react'

import { useApp } from '../../contexts/app-context'
import { useMcp } from '../../contexts/mcp-context'
import { usePlugin } from '../../contexts/plugin-context'
import { useSettings } from '../../contexts/settings-context'
import {
  LLMAPIKeyInvalidException,
  LLMAPIKeyNotSetException,
  LLMBaseUrlNotSetException,
  LLMModelNotFoundException,
} from '../../core/llm/exception'
import { getChatModelClient } from '../../core/llm/manager'
import { ChatMessage } from '../../types/chat'
import { PromptGenerator } from '../../utils/chat/promptGenerator'
import { ResponseGenerator } from '../../utils/chat/responseGenerator'
import { ErrorModal } from '../modals/ErrorModal'

type UseChatStreamManagerParams = {
  setChatMessages: React.Dispatch<React.SetStateAction<ChatMessage[]>>
  autoScrollToBottom: () => void
  promptGenerator: PromptGenerator
}

export type UseChatStreamManager = {
  abortActiveStreams: () => void
  detachActiveStream: (conversationId: string) => void
  submitChatMutation: UseMutationResult<
    void,
    Error,
    { chatMessages: ChatMessage[]; conversationId: string }
  >
}

export function useChatStreamManager({
  setChatMessages,
  autoScrollToBottom,
  promptGenerator,
}: UseChatStreamManagerParams): UseChatStreamManager {
  const app = useApp()
  const plugin = usePlugin()
  const { settings, setSettings } = useSettings()
  const { getMcpManager } = useMcp()

  const activeStreamAbortControllersRef = useRef<AbortController[]>([])
  const activeResponseGeneratorRef = useRef<ResponseGenerator | null>(null)
  const activeBaseMessagesRef = useRef<ChatMessage[]>([])
  const activeConversationIdRef = useRef<string | null>(null)

  const abortActiveStreams = useCallback(() => {
    for (const abortController of activeStreamAbortControllersRef.current) {
      abortController.abort()
    }
    activeStreamAbortControllersRef.current = []
    activeResponseGeneratorRef.current = null
    activeBaseMessagesRef.current = []
    activeConversationIdRef.current = null
  }, [])

  /**
   * Hand the active stream to the StreamStateManager without aborting it.
   * Called when the sidebar closes while streaming.
   */
  const detachActiveStream = useCallback(
    (conversationId: string) => {
      const generator = activeResponseGeneratorRef.current
      if (!generator || activeConversationIdRef.current !== conversationId) {
        return
      }

      // Get current response messages by reading the latest from setChatMessages
      // We need to extract the response messages (everything after baseMessages)
      let currentResponseMessages: ChatMessage[] = []
      setChatMessages((current) => {
        const baseLen = activeBaseMessagesRef.current.length
        currentResponseMessages = current.slice(baseLen)
        return current // Don't modify state
      })

      plugin.streamStateManager.detachStream(
        conversationId,
        activeBaseMessagesRef.current,
        generator,
        currentResponseMessages,
      )

      // Clear local refs but don't abort
      activeResponseGeneratorRef.current = null
      activeBaseMessagesRef.current = []
      activeConversationIdRef.current = null
    },
    [plugin.streamStateManager, setChatMessages],
  )

  const { providerClient, model } = useMemo(() => {
    try {
      return getChatModelClient({
        settings,
        modelId: settings.chatModelId,
      })
    } catch (error) {
      if (error instanceof LLMModelNotFoundException) {
        if (settings.chatModels.length === 0) {
          throw error
        }
        // Fallback to the first chat model if the selected chat model is not found
        const firstChatModel = settings.chatModels[0]
        setSettings({
          ...settings,
          chatModelId: firstChatModel.id,
          chatModels: settings.chatModels.map((model) =>
            model.id === firstChatModel.id
              ? {
                  ...model,
                  enable: true,
                }
              : model,
          ),
        })
        return getChatModelClient({
          settings,
          modelId: firstChatModel.id,
        })
      }
      throw error
    }
  }, [settings, setSettings])

  const submitChatMutation = useMutation({
    mutationFn: async ({
      chatMessages,
      conversationId,
    }: {
      chatMessages: ChatMessage[]
      conversationId: string
    }) => {
      const lastMessage = chatMessages.at(-1)
      if (!lastMessage) {
        // chatMessages is empty
        return
      }

      abortActiveStreams()
      const abortController = new AbortController()
      activeStreamAbortControllersRef.current.push(abortController)

      let unsubscribeResponseGenerator: (() => void) | undefined

      try {
        const mcpManager = await getMcpManager()
        const responseGenerator = new ResponseGenerator({
          providerClient,
          model,
          messages: chatMessages,
          conversationId,
          enableTools: settings.chatOptions.enableTools,
          maxAutoIterations: settings.chatOptions.maxAutoIterations,
          promptGenerator,
          mcpManager,
          abortSignal: abortController.signal,
        })

        // Track the active stream for potential detachment
        activeResponseGeneratorRef.current = responseGenerator
        activeBaseMessagesRef.current = chatMessages
        activeConversationIdRef.current = conversationId

        unsubscribeResponseGenerator = responseGenerator.subscribe(
          (responseMessages) => {
            setChatMessages((prevChatMessages) => {
              const lastMessageIndex = prevChatMessages.findIndex(
                (message) => message.id === lastMessage.id,
              )
              if (lastMessageIndex === -1) {
                // The last message no longer exists in the chat history.
                // This likely means a new message was submitted while this stream was running.
                // Abort this stream and keep the current chat history.
                abortController.abort()
                return prevChatMessages
              }
              return [
                ...prevChatMessages.slice(0, lastMessageIndex + 1),
                ...responseMessages,
              ]
            })
            requestAnimationFrame(() => autoScrollToBottom())
          },
        )

        await responseGenerator.run()
      } catch (error) {
        // Ignore AbortError
        if (error instanceof Error && error.name === 'AbortError') {
          return
        }
        throw error
      } finally {
        if (unsubscribeResponseGenerator) {
          unsubscribeResponseGenerator()
        }
        activeStreamAbortControllersRef.current =
          activeStreamAbortControllersRef.current.filter(
            (controller) => controller !== abortController,
          )

        // Notify StreamStateManager that this stream is complete
        plugin.streamStateManager.markComplete(conversationId)

        // Clear tracking refs
        if (activeConversationIdRef.current === conversationId) {
          activeResponseGeneratorRef.current = null
          activeBaseMessagesRef.current = []
          activeConversationIdRef.current = null
        }
      }
    },
    onError: (error) => {
      if (
        error instanceof LLMAPIKeyNotSetException ||
        error instanceof LLMAPIKeyInvalidException ||
        error instanceof LLMBaseUrlNotSetException
      ) {
        new ErrorModal(app, 'Error', error.message, error.rawError?.message, {
          showSettingsButton: true,
        }).open()
      } else {
        new Notice(error.message)
        console.error('Failed to generate response', error)
      }
    },
  })

  return {
    abortActiveStreams,
    detachActiveStream,
    submitChatMutation,
  }
}
